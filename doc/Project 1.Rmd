---
title: "Some Simple SPOOKY Data Analysis"
author: "Huijun Cui"
date: "January 31, 2018"
output:
  html_document: default
  pdf_document: default
---
I. Prerequisite
1. Setup the libraries
```{r, message = F, warning = F}
packages.used <- c("ggplot2", "dplyr", "tibble", "tidyr",  "stringr", "tidytext", "topicmodels", "wordcloud", "ggridges","igraph","tweenr","ggraph","scales")

# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

# install additional packages
if(length(packages.needed) > 0) {
  install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}

library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
#install.packages("topicmodels")
library(topicmodels)
library(wordcloud)
library(ggridges)
#install.packages("igraph")
library(igraph)
#install.packages("tweenr")
#library(tweenr)
#install.packages("ggraph")
library(ggraph)
library(scales)

source("../libs/multiplot.R")
```

2. Read in the data
```{r}
spk <- read.csv('../data/spooky.csv', as.is = TRUE)
```

3. An overview of the data structure and content
```{r}
head(spk)
summary(spk)
```

Each row of the dataset contains a unique ID, a single sentence text excerpt, and an abbreviated author name. `HPL` is Lovecraft, `MWS` is Shelly, and `EAP` is Poe.  
```{r}
sum(is.na(spk))
spk$author <- as.factor(spk$author)
```
Thus, there are no missing values. And the author name is transformed to be a factor variable.

II. Data Cleaning

The `unnest_tokens()` function to drop all punctuation and transform all words into lower case.  At least for now, the punctuation isn't really important to our analysis -- we want to study the words.  In addition, `tidytext` contains a dictionary of stop words, like "and" or "next", that we will get rid of for our analysis, the idea being that the non-common words (...maybe the SPOOKY words) that the authors use will be more interesting.  

```{r}
#library(janeaustenr)
library(dplyr)
library(stringr)

spk_byauthor <- spk %>%
  group_by(author) %>%
  mutate(linenumber = row_number()) %>%
  ungroup()
spkline <- spk %>%
  mutate(line = row_number())
  
spk_wrd <- unnest_tokens(spk_byauthor, word, text)
spk_wrdn <- unnest_tokens(spkline, word, text) %>%
  group_by(line)%>%
  mutate(wordorder = row_number())%>%
  ungroup
spk_wrd <- spk_wrd %>% anti_join(stop_words)
```

III. Data analysis  

1. Stop word analysis 
```{r}
#library(janeaustenr)
library(dplyr)
library(stringr)

spk_byauthor <- spk %>%
  group_by(author) %>%
  mutate(linenumber = row_number()) %>%
  ungroup()

spk_wrd <- unnest_tokens(spk_byauthor, word, text)
spk_stp <- unnest_tokens(spk_byauthor, word, text) %>% semi_join(stop_words)

author_words <- count(group_by(spk_stp, word, author))

all_words    <- rename(count(group_by(spk_stp, word)), all = n)
author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
author_words  
#png("../figs/stopword_frequency.png")
ggplot(author_words) +
  geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")

summary(spk_wrd)
summary(spk_byauthor)
```
The picture shows the habits of the three writers when they use stop words, and the second table provides the precise values of frequency of stop words. In the table, n is the stop word frequency of each author, while all refers to the frequency of all three authors. From these materials, we can find generally authors have similar tendency in stop word using, but typically EAP like to utilize stop words more than other two authors(especially in the example of word 'the'). However, the summary also shows that there are more sentences and words from EAP in the dataset, therefore the distribution may also result from this quantitative superiority. 

2. First word analysis
```{r}
spkline <- spk %>%
  mutate(line = row_number())
  
spk_wrdn <- unnest_tokens(spkline, word, text) %>%
  group_by(line)%>%
  mutate(wordorder = row_number())%>%
  ungroup

first_wrd <- filter(spk_wrdn, wordorder == 1)
first_wrd
#png("../figs/firstword_distribution.png")
first_wrd %>%
  group_by(author)%>%
  count(word, sort = TRUE) %>%
  head(20) %>% 
  mutate(first = reorder(word, n)) %>%
  ggplot(aes(first, n, fill = author)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author)
```
The table above shows the occupation modes of the first word of each sentence, and the table describes the first word(wih the top 20 frequencies ) utilization distribution of each authors. They show that most of first words are stop words, so the word distribution also corresponds to the results in stop word analysis -- the words EAP utilized as first words occupies more percentage in the most popular first word group.   

3. Word frequency analysis
```{R}
library(ggplot2)

spk_wrd <- spk_wrd %>% anti_join(stop_words)
spk_count <- spk_wrd %>%
  group_by(author) %>%
  count(word, sort = TRUE) %>%
  ungroup
#png("../figs/topwords.png")
spk_count %>%
  filter(n > 200) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = author)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
spk_count
```
These gragh and table give us the description of word analysis without consideration about stop words. From both the gragh and the table, it is obvious that MWS contributes most words (no stop word) in the dataset, though for some special cases like 'time', 'night', 'found' and etc, EAP and HPL dominate more. From this phenomena we can induce that MWS does not like to use stop words compared with other two authors.   

```{r}

spk_eap <- filter(spk_wrd, author == "EAP")
spk_hpl <- filter(spk_wrd, author == "HPL")
spk_mws <- filter(spk_wrd, author == "MWS")

spk_eap %>%
  count(word, sort = TRUE)
spk_hpl %>%
  count(word, sort = TRUE)
spk_mws %>%
  count(word, sort = TRUE)

# Counts number of times each author used each word.
author_words <- count(group_by(spk_wrd, word, author))

# Counts number of times each word was used.
all_words    <- rename(count(group_by(spk_wrd, word)), all = n)

author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
#png("../figs/topword_distribution.png")  
ggplot(author_words) +
  geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")
```
This part is about the detailed word profiles of the authors. We can see the wording preference of difference writers, like MWS tends to talk about life, while HPL and EAP prefer to discuss about time, MWS loves to mention love, while the other two authors are not. The first table is the word and its frequency list of EAP, the second table is for HPL, and the third one is for MWS.  

```{r}
# Words is a list of words, and freqs their frequencies
spk_wrd <- spk_wrd %>% anti_join(stop_words)
wordstotal <- count(group_by(spk_wrd, word))$word
freqstotal <- count(group_by(spk_wrd, word))$n

wordseap <- count(group_by(spk_eap, word))$word
freqseap <- count(group_by(spk_eap, word))$n

wordshpl <- count(group_by(spk_hpl, word))$word
freqshpl <- count(group_by(spk_hpl, word))$n

wordsmws <- count(group_by(spk_mws, word))$word
freqsmws <- count(group_by(spk_mws, word))$n

#png("../figs/Wordcloud_all.png")
wordcloud(wordstotal, freqstotal, max.words = 35, color = c("orange", "lightblue","grey")) 
#png("../figs/Wordcloud_eap.png")
wordcloud(wordseap, freqseap, max.words = 35, color = c("orange", "lightblue","grey")) 
#png("../figs/Wordcloud_hpl.png")
wordcloud(wordshpl, freqshpl, max.words = 35, color = c("orange", "lightblue","grey")) 
#png("../figs/Wordcloud_mws.png")
wordcloud(wordsmws, freqsmws, max.words = 35, color = c("orange", "lightblue","grey")) 
#dev.off()
```
Here are the word cloud graphs, in whih the 35 most common words in the entire datset and personal  dataset of each writer are plotted. It is very intuitionistic that "time", "life", and "night" all appear frequently.

4. Correlation analysis from data of different authoers
```{R}
frequency <- spk_wrd %>% 
  #extract words from possible italics
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>%
  gather(author, proportion,'EAP':'HPL')

frequency
library(scales)

# expect a warning about rows with missing values being removed
#png("../figs/frequencycorrelationMWS.png")
ggplot(frequency, aes(x = proportion, y = MWS, color = abs(MWS - proportion))) +
  geom_abline(color = "gray10", lty = 1) +
  geom_jitter(alpha = 0.05, color = 12, size = 3.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "MWS", x = NULL)
```
```{R}
frequency2 <- spk_wrd %>% 
  #extract words from possible italics
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>%
  gather(author, proportion,'MWS':'HPL')

library(scales)
# expect a warning about rows with missing values being removed
#png("../figs/frequencycorrelationeap.png")
ggplot(frequency2, aes(x = proportion, y = EAP, color = abs(EAP - proportion))) +
  geom_abline(color = "gray10", lty = 1) +
  geom_jitter(alpha = 0.05, color = 12, size = 3.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "EAP", x = NULL)
```
Here are the word frequency comparison graphs between EAP&HPL, EAP&MWS, MWS&HPL, in which the coordinate values represent the proportion of words in the text groups of different authors. From the graphs, the word points close to the abline if they have similar proportion value in the text groups of different authors. 
For example, in the graph EAP&HPL and EAP&MWS, we can pick up the word 'together', which is above the abline on both of the graphs. This means 'together' has a relatively higher proportion in EAP's works than it in HPL&MWS's works.

```{r}
cor.test(data = frequency[frequency$author == "EAP",],
         ~ proportion + MWS)

cor.test(data = frequency[frequency$author == "HPL",],
         ~ proportion + MWS)

cor.test(data = frequency2[frequency2$author == "HPL",],
         ~ proportion + EAP)
```
The first correlation test is between EAP and MWS, the second one is between HPL and MWS, the last one is between HPL and EAP. These results show the correlation relationship between HPL&MWS is lower than that between HPL&EAP and that between MWS&EAP. 

5. TF-IDF

TF-IAF shows the relative frequency a certain author uses a word compared with that all the authors use the word, and this can be regarded as a more detailed edition of the last part.

```{r}
frequency <- count(spk_wrd, author, word)
tf_idf    <- bind_tf_idf(frequency, word, author, n)
head(tf_idf)
tail(tf_idf)

tf_idf    <- arrange(tf_idf, desc(tf_idf))
tf_idf    <- mutate(tf_idf, word = factor(word, levels = rev(unique(word))))

tf_idf_25 <- top_n(tf_idf, 25, tf_idf)
#png("../figs/tf_idf_25.png")
ggplot(tf_idf_25) +
  geom_col(aes(word, tf_idf, fill = author)) +
  labs(x = NULL, y = "TF-IDF values") +
  theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))
#png("../figs/tf_idf_25sep.png")  
ggplot(tf_idf_25) +
  geom_col(aes(word, tf_idf, fill = author)) +
  labs(x = NULL, y = "TF-IDF values") +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")
```
The first table is the head part of TF-IDF distribution table. and the second one is the tail part of the TF-IDF list. From the distribution histogram, the typical words are usually names or nouns, which are related to different topics and contents different author like to discuss. They can be regarded as signs to recognize who the author is of a certain text.

6. Sentiment Analysis

```{r}
#library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)

samplesentiment <- spk_wrd %>%
  inner_join(get_sentiments("bing")) %>%
  count(author, index = linenumber %/% 50, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

library(ggplot2)
#png("../figs/sentimentbing.png")

ggplot(samplesentiment, aes(index, sentiment, fill = author)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~author, ncol = 2, scales = "free_x")

```
Here I use Bing et al. as the lexicon to analyse the emotional bias. I use the value of positive - negative as the representative of sentiment, and put every 50 sentences as a group to make analysis. From the graphs above, we can conclude most of the time all of the authors discussed about negative things, especially HPL.
```{r}
bing_word_counts <- spk_wrd %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, author, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
#png("../figs/sentimenttopword.png")
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(12) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~author, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()

```
Here are the distribution of emotional words. The table shows the words, their sentiment categories, and the quantities, and their author. The graph is more straightforward, and it shows the words with sentiment of top 12 frequencies in the whole dataset. We can observe that MWS use most sentimental words, and in contrary, HPL and EAP have a more calm and cold writing style. Besides, MWS prefer to use more warm and positive words, while the other two authors like to use negative words.

```{r}
afinneap <- spk_eap %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 50) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")

bing_and_nrceap <- bind_rows(spk_eap %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          spk_eap %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 50, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
#png("../figs/sentimentcompeap.png")
bind_rows(afinneap, 
          bing_and_nrceap) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  ggtitle("Comparison in three sentiment lexicons by EAP", subtitle = NULL)

afinnhpl <- spk_hpl %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 50) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")

bing_and_nrchpl <- bind_rows(spk_hpl %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          spk_hpl %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 50, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
#png("../figs/sentimentcomphpl.png")
bind_rows(afinnhpl, 
          bing_and_nrchpl) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  ggtitle("Comparison in three sentiment lexicons by HPL", subtitle = NULL)

afinnmws <- spk_mws %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 50) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")

#png("../figs/sentimentcompmws.png")
bing_and_nrcmws <- bind_rows(spk_mws %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          spk_mws %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 50, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(afinnmws, 
          bing_and_nrcmws) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  ggtitle("Comparison in three sentiment lexicons by MWS", subtitle = NULL)

get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)
```
Here are the comparison of three diferent sentimental evaluation methods. So basically we can see they shows similar tendency in their discription, but with different sentimental scores. Usually Bing procides the lowest scores, and NRC gives us the highest scores. The last two tables show Bing has more negative words than NRC, and  this may be the reason of the discrepancy.  
Also fro the graphs, HPL shows stable negative psychological state, MWS and EAP are similar, and EAP's emotion is more turbulent than other's. 

7. Relationships between 2-grams
```{r}
library(dplyr)
library(tidytext)
#library(janeaustenr)

eap_sentence <- filter(spk_byauthor, author == "EAP")
hpl_sentence <- filter(spk_byauthor, author == "HPL")
mws_sentence <- filter(spk_byauthor, author == "MWS")

eap_bigrams <- eap_sentence %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
hpl_bigrams <- hpl_sentence %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
mws_bigrams <- mws_sentence %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
spk_bigrams <- spk_byauthor %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

eap_bigrams %>%
  count(bigram, sort = TRUE)
hpl_bigrams %>%
  count(bigram, sort = TRUE)
mws_bigrams %>%
  count(bigram, sort = TRUE)
spk_bigrams %>%
  count(bigram, sort = TRUE)

bigrams_separated <- spk_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigram_counts_au <- bigrams_filtered %>% 
  group_by(author) %>%
  count(word1, word2, sort = TRUE) %>%
  ungroup 
bigram_counts_au
```
The first three tables are bigram list of EAP, HPL and MWS, which show bigrams and their frequencies. The fourth table is a summary table of the whole dataset. Form the table, it is clear that all of authors like to use stop words 'of the', 'in the', 'to the' and etc. The last table is the bigram distribution without consideration of stop words. It shows usually MWS uses bigram to mention people, while others tend to use more modal particles like 'ha ha'. 
```{r}
bigram_tf_idf <- bigrams_united %>%
  count(author, bigram) %>%
  bind_tf_idf(bigram, author, n) %>%
  arrange(desc(tf_idf))
#png("../figs/bigramtf.png")
bigram_tf_idf %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(author) %>% 
  top_n(8) %>% 
  ungroup %>%
  ggplot(aes(bigram, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip()
```
They are a more straightforward presentation of the last part. Each graph shows the top bigrams each author like to use, and the bigrams also leak some details of their stories.  
```{r}
AFINN <- get_sentiments("afinn")
AFINN

negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()
negated_words
#png("../figs/negbigram.png")

negated_words %>%
  mutate(contribution = n * score) %>%
  group_by(word1) %>% 
  arrange(desc(abs(contribution))) %>%
  head(30) %>%
  ungroup %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  ylab("Sentiment score * number of occurrences") +
  facet_wrap(~word1, ncol = 4, scales = "free") +
  coord_flip()
```
Because negation adjectives can affect the sentiment of a text, I pick up never, no, not, without to find the bigrams combined by them and sentimental words to control the influences. The lexiton utilized is afinn.   
The picture above shows in the whole dataset, most of time negators are connected with a positive sentimental words. This phenomenon explains the negative sentimental style of the authors, and make us induce that may be the conclusion in the sentimental analysis is not accurate.
```{r}
#EAP
eapbigrams_sep <- eap_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

negation_words <- c("not", "no", "never", "without")

negated_words <- eapbigrams_sep %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()
#png("../figs/negbigrameap.png")
negated_words %>%
  mutate(contribution = n * score) %>%
  group_by(word1) %>% 
  arrange(desc(abs(contribution))) %>%
  head(30) %>%
  ungroup %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  ylab("Sentiment score * number of occurrences of EAP") +
  facet_wrap(~word1, ncol = 4, scales = "free") +
  coord_flip()

#HPL
hplbigrams_sep <- hpl_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

negation_words <- c("not", "no", "never", "without")

negated_words <- hplbigrams_sep %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()
#png("../figs/negbigramhpl.png")
negated_words %>%
  mutate(contribution = n * score) %>%
  group_by(word1) %>% 
  arrange(desc(abs(contribution))) %>%
  head(30) %>%
  ungroup %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  ylab("Sentiment score * number of occurrences of HPL") +
  facet_wrap(~word1, ncol = 4, scales = "free") +
  coord_flip()

#MWS
mwsbigrams_sep <- mws_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

negation_words <- c("not", "no", "never", "without")

negated_words <- mwsbigrams_sep %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()
#png("../figs/negbigrammws.png")
negated_words %>%
  mutate(contribution = n * score) %>%
  group_by(word1) %>% 
  arrange(desc(abs(contribution))) %>%
  head(30) %>%
  ungroup %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  ylab("Sentiment score * number of occurrences of MWS") +
  facet_wrap(~word1, ncol = 4, scales = "free") +
  coord_flip() 
```
This part is a detailed edition of the last graph. These three pictures can verify the results we gained from the last part, because the works of every author show similar tendency. And they also provide some proofs that EAP nd MWS don't like to use without in comparison with HPL.
```{r}
library(igraph)
bigram_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph
set.seed(2016)
library(ggraph)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
#png("../figs/bigraph.png")
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "purple", size = 4) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

bigrams_eapsep <- eap_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_eapfilt <- bigrams_eapsep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_countseap <- bigrams_eapfilt %>% 
  count(word1, word2, sort = TRUE)

bigram_grapheap <- bigram_countseap %>%
  filter(n > 10) %>%
  graph_from_data_frame()

set.seed(2016)
library(ggraph)
#png("../figs/bigrapheap.png")
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_grapheap, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "red", size = 4) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

bigrams_hplsep <- hpl_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_hplfilt <- bigrams_hplsep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_countshpl <- bigrams_hplfilt %>% 
  count(word1, word2, sort = TRUE)

bigram_graphhpl <- bigram_countshpl %>%
  filter(n > 10) %>%
  graph_from_data_frame()
set.seed(2016)
library(ggraph)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
#png("../figs/bigraphhpl.png")
ggraph(bigram_graphhpl, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "blue", size = 4) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

bigrams_mwssep <- mws_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_mwsfilt <- bigrams_mwssep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_countsmws <- bigrams_mwsfilt %>% 
  count(word1, word2, sort = TRUE)

bigram_graphmws <- bigram_countsmws %>%
  filter(n > 10) %>%
  graph_from_data_frame()

set.seed(2016)
library(ggraph)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
#png("../figs/bigraphmws.png")
ggraph(bigram_graphmws, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "green", size = 4) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```
Here are the graphs show the most popular bigrams htroughout the dataset, and the contribution from different authors. Arrows represent the inner directions of bigrams. 
The first graph discribes the total dataset, the second one is for EAP, the third one is for HPL. the forth one is for MWS.
From the comparison, we can see EAP uses the most fixed bigrams, while MWS uses the least. But this may also because the discrepancies between total quantities of words from different authors. Besides, EAP mentions more daily supplies, HPL mentions more locations,  while MWS mentions more nouns about a country. We can induce that MWS concnetrates on kingdoms (also is a sign of her era), HPL likes to talk about location and nature, while EAP writes stories occuring indoors.

8. Topic Models
We use the `topicmodels` package for this analysis.  Since the `topicmodels` package doesn't use the `tidytext` framework, we first convert our `spooky_wrd` dataframe into a document term matrix (DTM) matrix using `tidytext` tools.

```{r}
# Counts how many times each word appears in each sentence
spk_wrd <- unnest_tokens(spk, word, text)
spk_wrd <- anti_join(spk_wrd, stop_words, by = "word")
swrd_freqs <- count(spk_wrd, id, word)
head(swrd_freqs)

# Creates a DTM matrix
spk_wrd_tm <- cast_dtm(swrd_freqs, id, word, n)
spk_wrd_tm
length(unique(spk_wrd$id))
length(unique(spk_wrd$word))
```

The matrix `spooky_wrd_tm` is a sparse matrix with 19467 rows, corresponding to the 19467 ids (or originally, sentences) in the `spooky_wrd` dataframe, and 24941 columns corresponding to the total number of unique words in the `spooky_wrd` dataframe.  So each row of `spooky_wrd_tm` corresponds to one of the original sentences.  The value of the matrix at a certain position is then the number of occurences of that word (determined by the column) in this specific sentence (determined by the row).  Since most sentence/word pairings don't occur, the matrix is sparse meaning there are many zeros.

For LDA we must pick the number of possible topics.  Let's try 12, though this selection is admittedly arbitrary.

```{r}
spk_wrd_lda <- LDA(spk_wrd_tm, k = 2, control = list(seed = 1234))
spk_wrd_top <- tidy(spk_wrd_lda, matrix = "beta")
spk_wrd_top
```

## Topics Terms

We note that in the above we use the `tidy` function to extract the per-topic-per-word probabilities, called "beta" or $\beta$, for the model.  The final output has a one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic. For example, the term "content" has a $1.619628 \times 10^{-5}$ probability of being generated from topic 4.  We visualize the top terms (meaning the most likely terms associated with each topic) in the following.

```{r}
# Grab the top five words for each topic.
spk_wrd_top_4 <- ungroup(top_n(group_by(spk_wrd_top, topic), 4, beta))
spk_wrd_top_4 <- arrange(spk_wrd_top_4, topic, -beta)
spk_wrd_top_4 <- mutate(spk_wrd_top_4, term = reorder(term, beta))

ggplot(spk_wrd_top_4) +
  geom_col(aes(term, beta, fill = factor(topic)), show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip()
```

In the above, we see that the first topic is characterized by words like "love", "earth", and "words" while the third topic includes the word "thousand", and the fifth topic the word "beauty".  Note that the words "eyes" and "time" appear in many topics.  This is the advantage to topic modelling as opposed to clustering when using natural language -- often a word may be likely to appear in documents characterized by multiple topics.

We can also study terms that have the greatest difference in probabilities between the topics, ignoring the words that are shared with similar frequency between topics. We choose only the first 3 topics as example and visualise the differences by plotting log ratios: $log_{10}(\beta \text{ of topic x }/ \beta \text{ of topic y})$. So if a word is 10 times more frequent in topic x the log ratio will be 1, whereas it will be -1 if the word is 10 times more frequent in topic y. 

```{r}
beta_spread_12 <- spk_wrd_top %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) 

beta_spread_12 <- group_by(beta_spread_12, direction = log_ratio > 0)
beta_spread_12 <- ungroup(top_n(beta_spread_12, 5, abs(log_ratio)))
beta_spread_12 <- mutate(beta_spread_12, term = reorder(term, log_ratio))

lr12 <- ggplot(beta_spread_12) +
      geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
      theme(legend.position = "none") +
      labs(y = "Log ratio of beta in topic 2 / topic 1") +
      coord_flip()

beta_spread_13 <- spk_wrd_top %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) 

beta_spread_13 <- group_by(beta_spread_13, direction = log_ratio > 0)
beta_spread_13 <- ungroup(top_n(beta_spread_13, 5, abs(log_ratio)))
beta_spread_13 <- mutate(beta_spread_13, term = reorder(term, log_ratio))

lr13 <- ggplot(beta_spread_13) +
      geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
      theme(legend.position = "none") +
      labs(y = "Log ratio of beta in topic 3 / topic 1") +
      coord_flip()

beta_spread_23 <- spk_wrd_top %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) 

beta_spread_23 <- group_by(beta_spread_23, direction = log_ratio > 0)
beta_spread_23 <- ungroup(top_n(beta_spread_23, 5, abs(log_ratio)))
beta_spread_23 <- mutate(beta_spread_23, term = reorder(term, log_ratio))

lr23 <- ggplot(beta_spread_23) +
      geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
      theme(legend.position = "none") +
      labs(y = "Log ratio of beta in topic 3 / topic 2") +
      coord_flip()
```
In the above, the words more common to topic 2 than topic 1 are "moon", "air", and "window" while the words more common to topic 1 are "moment", "marie", and "held".

## Sentence Topics

Above we look at the words representing each topic, we can also study the topics representing each documents, or in our case sentence.  We use the `tidy` function to extract the per-document-per-topic probabilities, called "gamma" or $\gamma$, for the model.

```{r}
spk_wrd_docs <- tidy(spk_wrd_lda, matrix = "gamma")
spk_wrd_docs
```

The above table holds the estimated proportion of words from that sentence (id) that are generated from that topic. For example, the model estimates that only about 8.301% of the words in sentence id00001 were generated from topic 1.

```{r}
author_top <- left_join(spk_wrd_docs, spk, by = c("document" = "id"))
author_top <- select(author_top, -text)
author_top$topic <- as.factor(author_top$topic)

# Chooses the top topic per sentence
author_top <- ungroup(top_n(group_by(author_top, document), 1, gamma))

# Counts the number of sentences represented by each topic per author 
author_top <- ungroup(count(group_by(author_top, author, topic)))

author_top

ggplot(author_top) +
  geom_col(aes(topic, n, fill = factor(topic)), show.legend = FALSE) +
  facet_wrap(~ author, scales = "free", ncol = 4) +
  coord_flip()
```
